allow_missing_params: false
batch_size: 1024
batch_type: word
bucket_width: 10
checkpoint_frequency: 20000
cnn_activation_type: glu
cnn_hidden_dropout: 0.2
cnn_kernel_width:
- 3
- 3
cnn_num_hidden: 512
cnn_positional_embedding_type: learned
cnn_project_qkv: false
config: null
conv_embed_add_positional_encodings: false
conv_embed_dropout: 0.0
conv_embed_max_filter_width: 8
conv_embed_num_filters:
- 200
- 200
- 250
- 250
- 300
- 300
- 300
- 300
conv_embed_num_highway_layers: 4
conv_embed_output_dim: null
conv_embed_pool_stride: 5
decode_and_evaluate: 300
decode_and_evaluate_device_id: null
decode_and_evaluate_use_cpu: false
decoder: transformer
decoder_only: false
device_ids:
- 0
- 1
disable_device_locking: true
dry_run: false
embed_dropout:
- 0.0
- 0.0
embed_weight_init: default
encoder: transformer
fill_up: replicate
fixed_param_names: []
gradient_clipping_threshold: -1.0
gradient_clipping_type: none
gradient_compression_threshold: 0.5
gradient_compression_type: null
initial_learning_rate: 0.0002
keep_last_params: 10
kvstore: device
label_smoothing: 0.1
layer_normalization: false
learning_rate_decay_optimizer_states_reset: best
learning_rate_decay_param_reset: true
learning_rate_half_life: 10
learning_rate_reduce_factor: 0.9
learning_rate_reduce_num_not_improved: 8
learning_rate_schedule: null
learning_rate_scheduler_type: plateau-reduce
learning_rate_warmup: 0
lhuc: null
lock_dir: /tmp
loss: cross-entropy
loss_normalization_type: valid
max_num_checkpoint_not_improved: 32
max_num_epochs: null
max_samples: null
max_seq_len:
- 99
- 99
max_updates: null
metrics:
- perplexity
min_num_epochs: null
min_samples: null
min_updates: null
momentum: null
monitor_pattern: null
monitor_stat_func: mx_default
no_bucketing: false
num_embed:
- 512
- 512
num_layers:
- 6
- 6
num_words:
- 0
- 0
optimized_metric: perplexity
optimizer: adam
optimizer_params: null
output: ../../mono_model_11_21_trans_mul
overwrite_output: false
pad_vocab_to_multiple_of: null
params: null
prepared_data: ../../mono_11_21
quiet: false
rnn_attention_coverage_num_hidden: 1
rnn_attention_coverage_type: count
rnn_attention_in_upper_layers: false
rnn_attention_mhdot_heads: null
rnn_attention_num_hidden: null
rnn_attention_type: mlp
rnn_attention_use_prev_word: false
rnn_cell_type: lstm
rnn_context_gating: false
rnn_decoder_hidden_dropout: 0.2
rnn_decoder_state_init: last
rnn_dropout_inputs:
- 0.0
- 0.0
rnn_dropout_recurrent:
- 0.0
- 0.0
rnn_dropout_states:
- 0.0
- 0.0
rnn_enc_last_hidden_concat_to_embedding: false
rnn_encoder_reverse_input: false
rnn_first_residual_layer: 2
rnn_forget_bias: 0.0
rnn_h2h_init: orthogonal
rnn_num_hidden: 1024
rnn_residual_connections: false
rnn_scale_dot_attention: false
seed: 13
shared_vocab: false
source: null
source_factor_vocabs: []
source_factors: []
source_factors_num_embed:
- 4
- 4
source_vocab: null
target: null
target_vocab: null
transformer_activation_type: relu
transformer_attention_heads:
- 8
- 8
transformer_dropout_act: 0.1
transformer_dropout_attention: 0.1
transformer_dropout_prepost: 0.1
transformer_feed_forward_num_hidden:
- 2048
- 2048
transformer_model_size:
- 512
- 512
transformer_positional_embedding_type: fixed
transformer_postprocess:
- dr
- dr
transformer_preprocess:
- n
- n
use_cpu: false
validation_source: /home/edwardhu/export/paraBank/data/ParaBank/training_data/parabank_11_9/parabank_test.src.bpe
validation_source_factors:
- /home/edwardhu/export/paraBank/data/ParaBank/training_data/parabank_11_9/parabank_test.factor.bpe
- /home/edwardhu/export/paraBank/data/ParaBank/training_data/parabank_11_9/parabank_test.factor.cap
validation_target: /home/edwardhu/export/paraBank/data/ParaBank/training_data/parabank_11_9/parabank_test.trg.bpe
weight_decay: 0.0
weight_init: xavier
weight_init_scale: 3.0
weight_init_xavier_factor_type: avg
weight_init_xavier_rand_type: uniform
weight_normalization: false
weight_tying: true
weight_tying_type: src_trg_softmax
word_min_count:
- 1
- 1
